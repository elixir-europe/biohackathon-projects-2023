@article {Lamprecht:2021,
	Title = {Perspectives on automated composition of workflows in the life sciences},
	Author = {Lamprecht, Anna-Lena and Palmblad, Magnus and Ison, Jon and Schwämmle, Veit and Al Manir, Mohammad Sadnan and Altintas, Ilkay and Baker, Christopher J O and Ben Hadj Amor, Ammar and Capella-Gutierrez, Salvador and Charonyktakis, Paulos and Crusoe, Michael R and Gil, Yolanda and Goble, Carole and Griffin, Timothy J and Groth, Paul and Ienasescu, Hans and Jagtap, Pratik and Kalaš, Matúš and Kasalica, Vedran and Khanteymoori, Alireza and Kuhn, Tobias and Mei, Hailiang and Ménager, Hervé and Möller, Steffen and Richardson, Robin A and Robert, Vincent and Soiland-Reyes, Stian and Stevens, Robert and Szaniszlo, Szoke and Verberne, Suzan and Verhoeven, Aswin and Wolstencroft, Katherine},
	DOI = {10.12688/f1000research.54159.1},
	Volume = {10},
	Year = {2021},
	Journal = {F1000Research},
	ISSN = {2046-1402},
	Pages = {897},
	Abstract = {Scientific data analyses often combine several computational tools in automated pipelines, or workflows. Thousands of such workflows have been used in the life sciences, though their composition has remained a cumbersome manual process due to a lack of standards for annotation, assembly, and implementation. Recent technological advances have returned the long-standing vision of automated workflow composition into focus. This article summarizes a recent Lorentz Center workshop dedicated to automated composition of workflows in the life sciences. We survey previous initiatives to automate the composition process, and discuss the current state of the art and future perspectives. We start by drawing the "big picture" of the scientific workflow development life cycle, before surveying and discussing current methods, technologies and practices for semantic domain modelling, automation in workflow development, and workflow assessment. Finally, we derive a roadmap of individual and community-based actions to work toward the vision of automated workflow development in the forthcoming years. A central outcome of the workshop is a general description of the workflow life cycle in six stages: 1) scientific question or hypothesis, 2) conceptual workflow, 3) abstract workflow, 4) concrete workflow, 5) production workflow, and 6) scientific results. The transitions between stages are facilitated by diverse tools and methods, usually incorporating domain knowledge in some form. Formal semantic domain modelling is hard and often a bottleneck for the application of semantic technologies. However, life science communities have made considerable progress here in recent years and are continuously improving, renewing interest in the application of semantic technologies for workflow exploration, composition and instantiation. Combined with systematic benchmarking with reference data and large-scale deployment of production-stage workflows, such technologies enable a more systematic process of workflow development than we know today. We believe that this can lead to more robust, reusable, and sustainable workflows in the future.},
	URL = {https://europepmc.org/articles/PMC8573700},
}

@article {Kasalica:2021,
	Title = {APE in the Wild: Automated Exploration of Proteomics Workflows in the bio.tools Registry},
	Author = {Kasalica, Vedran and Schwämmle, Veit and Palmblad, Magnus and Ison, Jon and Lamprecht, Anna-Lena},
	DOI = {10.1021/acs.jproteome.0c00983},
	Number = {4},
	Volume = {20},
	Month = {April},
	Year = {2021},
	Journal = {Journal of proteome research},
	ISSN = {1535-3893},
	Pages = {2157—2165},
	Abstract = {The bio.tools registry is a main catalogue of computational tools in the life sciences. More than 17 000 tools have been registered by the international bioinformatics community. The bio.tools metadata schema includes semantic annotations of tool functions, that is, formal descriptions of tools' data types, formats, and operations with terms from the EDAM bioinformatics ontology. Such annotations enable the automated composition of tools into multistep pipelines or workflows. In this Technical Note, we revisit a previous case study on the automated composition of proteomics workflows. We use the same four workflow scenarios but instead of using a small set of tools with carefully handcrafted annotations, we explore workflows directly on bio.tools. We use the Automated Pipeline Explorer (APE), a reimplementation and extension of the workflow composition method previously used. Moving "into the wild" opens up an unprecedented wealth of tools and a huge number of alternative workflows. Automated composition tools can be used to explore this space of possibilities systematically. Inevitably, the mixed quality of semantic annotations in bio.tools leads to unintended or erroneous tool combinations. However, our results also show that additional control mechanisms (tool filters, configuration options, and workflow constraints) can effectively guide the exploration toward smaller sets of more meaningful workflows.},
	URL = {https://europepmc.org/articles/PMC8041394},
}

@article {Palmblad:2019,
	Title = {Automated workflow composition in mass spectrometry-based proteomics},
	Author = {Palmblad, Magnus and Lamprecht, Anna-Lena and Ison, Jon and Schwämmle, Veit},
	DOI = {10.1093/bioinformatics/bty646},
	Number = {4},
	Volume = {35},
	Month = {February},
	Year = {2019},
	Journal = {Bioinformatics (Oxford, England)},
	ISSN = {1367-4803},
	Pages = {656—664},
	Abstract = {&lt;h4&gt;Motivation&lt;/h4&gt;Numerous software utilities operating on mass spectrometry (MS) data are described in the literature and provide specific operations as building blocks for the assembly of on-purpose workflows. Working out which tools and combinations are applicable or optimal in practice is often hard. Thus researchers face difficulties in selecting practical and effective data analysis pipelines for a specific experimental design.&lt;h4&gt;Results&lt;/h4&gt;We provide a toolkit to support researchers in identifying, comparing and benchmarking multiple workflows from individual bioinformatics tools. Automated workflow composition is enabled by the tools' semantic annotation in terms of the EDAM ontology. To demonstrate the practical use of our framework, we created and evaluated a number of logically and semantically equivalent workflows for four use cases representing frequent tasks in MS-based proteomics. Indeed we found that the results computed by the workflows could vary considerably, emphasizing the benefits of a framework that facilitates their systematic exploration.&lt;h4&gt;Availability and implementation&lt;/h4&gt;The project files and workflows are available from https://github.com/bio-tools/biotoolsCompose/tree/master/Automatic-Workflow-Composition.&lt;h4&gt;Supplementary information&lt;/h4&gt;Supplementary data are available at Bioinformatics online.},
	URL = {https://europepmc.org/articles/PMC6378944},
}

@INPROCEEDINGS{Prasser:2014,
  author={Prasser, Fabian and Kohlmayer, Florian and Kuhn, Klaus A.},
  booktitle={2014 IEEE 27th International Symposium on Computer-Based Medical Systems}, 
  title={A Benchmark of Globally-Optimal Anonymization Methods for Biomedical Data}, 
  year={2014},
  volume={},
  number={},
  pages={66-71},
  doi={10.1109/CBMS.2014.85}}

@article{Pilan:2022,
    title = "The Text Anonymization Benchmark ({TAB}): A Dedicated Corpus and Evaluation Framework for Text Anonymization",
    author = "Pil{\'a}n, Ildik{\'o}  and
      Lison, Pierre  and
      {\O}vrelid, Lilja  and
      Papadopoulou, Anthi  and
      S{\'a}nchez, David  and
      Batet, Montserrat",
    journal = "Computational Linguistics",
    volume = "48",
    number = "4",
    month = dec,
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.cl-4.19",
    doi = "10.1162/coli_a_00458",
    pages = "1053--1101",
    abstract = "We present a novel benchmark and associated evaluation metrics for assessing the performance of text anonymization methods. Text anonymization, defined as the task of editing a text document to prevent the disclosure of personal information, currently suffers from a shortage of privacy-oriented annotated text resources, making it difficult to properly evaluate the level of privacy protection offered by various anonymization methods. This paper presents TAB (Text Anonymization Benchmark), a new, open-source annotated corpus developed to address this shortage. The corpus comprises 1,268 English-language court cases from the European Court of Human Rights (ECHR) enriched with comprehensive annotations about the personal information appearing in each document, including their semantic category, identifier type, confidential attributes, and co-reference relations. Compared with previous work, the TAB corpus is designed to go beyond traditional de-identification (which is limited to the detection of predefined semantic categories), and explicitly marks which text spans ought to be masked in order to conceal the identity of the person to be protected. Along with presenting the corpus and its annotation layers, we also propose a set of evaluation metrics that are specifically tailored toward measuring the performance of text anonymization, both in terms of privacy protection and utility preservation. We illustrate the use of the benchmark and the proposed metrics by assessing the empirical performance of several baseline text anonymization models. The full corpus along with its privacy-oriented annotation guidelines, evaluation scripts, and baseline models are available on: \url{https://github.com/NorskRegnesentral/text-anonymization-benchmark}.",
}

@article {Matthews:1975,
	Title = {Comparison of the predicted and observed secondary structure of T4 phage lysozyme},
	Author = {Matthews, BW},
	DOI = {10.1016/0005-2795(75)90109-9},
	Number = {2},
	Volume = {405},
	Month = {October},
	Year = {1975},
	Journal = {Biochimica et biophysica acta},
	ISSN = {0006-3002},
	Pages = {442—451},
	Abstract = {Predictions of the secondary structure of T4 phage lysozyme, made by a number of investigators on the basis of the amino acid sequence, are compared with the structure of the protein determined experimentally by X-ray crystallography. Within the amino terminal half of the molecule the locations of helices predicted by a number of methods agree moderately well with the observed structure, however within the carboxyl half of the molecule the overall agreement is poor. For eleven different helix predictions, the coefficients giving the correlation between prediction and observation range from 0.14 to 0.42. The accuracy of the predictions for both beta-sheet regions and for turns are generally lower than for the helices, and in a number of instances the agreement between prediction and observation is no better than would be expected for a random selection of residues. The structural predictions for T4 phage lysozyme are much less successful than was the case for adenylate kinase (Schulz et al. (1974) Nature 250, 140-142). No one method of prediction is clearly superior to all others, and although empirical predictions based on larger numbers of known protein structure tend to be more accurate than those based on a limited sample, the improvement in accuracy is not dramatic, suggesting that the accuracy of current empirical predictive methods will not be substantially increased simply by the inclusion of more data from additional protein structure determinations.},
	URL = {https://doi.org/10.1016/0005-2795(75)90109-9},
}

@article{Du:2023,
author = {Du, Xinsong and Dastmalchi, Farhad and Diller, Matthew A. and Brochhausen, Mathias and Garrett, Timothy J. and Hogan, William R. and Lemas, Dominick J.},
title = {An Automated Workflow Composition System for Liquid Chromatography–Mass Spectrometry Metabolomics Data Processing},
journal = {Journal of the American Society for Mass Spectrometry},
volume = {0},
number = {0},
pages = {null},
year = {0},
doi = {10.1021/jasms.3c00248},
    note ={PMID: 37874901},

URL = {https://doi.org/10.1021/jasms.3c00248},
eprint = {https://doi.org/10.1021/jasms.3c00248}

}

@software{Kasalica:2023a,
  author       = {Kasalica, Vedran and
                  Peter, Kok and
                  Nauman, Ahmed and
                  Felipe, Morato and
                  Anna-Lena, Lamprecht and
                  Magnus, Palmblad},
  title        = {{Workflomics: Bioinformatics Workflow Generation 
                   and Benchmarking}},
  month        = nov,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v.0.1.2},
  doi          = {10.5281/zenodo.10116067},
  url          = {https://doi.org/10.5281/zenodo.10116067}
}

@article{Ison:2016,
	title = {Tools and data services registry: a community effort to document bioinformatics resources},
	volume = {44},
	issn = {1362-4962},
	shorttitle = {Tools and data services registry},
	doi_comment = {10.1093/nar/gkv1116},
	abstract = {Life sciences are yielding huge data sets that underpin scientific discoveries fundamental to improvement in human health, agriculture and the environment. In support of these discoveries, a plethora of databases and tools are deployed, in technically complex and diverse implementations, across a spectrum of scientific disciplines. The corpus of documentation of these resources is fragmented across the Web, with much redundancy, and has lacked a common standard of information. The outcome is that scientists must often struggle to find, understand, compare and use the best resources for the task at hand.Here we present a community-driven curation effort, supported by ELIXIR-the European infrastructure for biological information-that aspires to a comprehensive and consistent registry of information about bioinformatics resources. The sustainable upkeep of this Tools and Data Services Registry is assured by a curation effort driven by and tailored to local needs, and shared amongst a network of engaged partners.As of November 2015, the registry includes 1785 resources, with depositions from 126 individual registrations including 52 institutional providers and 74 individuals. With community support, the registry can become a standard for dissemination of information about bioinformatics resources: we welcome everyone to join us in this common endeavour. The registry is freely available at https://bio.tools.},
	language = {eng},
	number = {D1},
	journal = {Nucleic Acids Research},
	author = {Ison, Jon and Rapacki, Kristoffer and M\'{e}nager, Herv\'{e}  and {others}},
	month = jan,
	year = {2016},
	pmid = {26538599},
	pmcid = {PMC4702812},
	keywords = {Computational Biology, Software, Data Curation, Registries, bio.tools},
	pages = {D38--47},
	annote = {biotoolsSchema},
	file = {Full Text:/home/vedran/Zotero/storage/JMCZ6HV8/Ison et al. - 2016 - Tools and data services registry a community effo.pdf:application/pdf}
}

@article {Capella-Gutierrez:2017,
	author = {Capella-Gutierrez, Salvador and Iglesia, Diana de la and Haas, Juergen and Lourenco, Analia and Fern{\'a}ndez, Jos{\'e} Mar{\'\i}a and Repchevsky, Dmitry and Dessimoz, Christophe and Schwede, Torsten and Notredame, Cedric and Gelpi, Josep Ll and Valencia, Alfonso},
	title = {Lessons Learned: Recommendations for Establishing Critical Periodic Scientific Benchmarking},
	elocation-id = {181677},
	year = {2017},
	doi = {10.1101/181677},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {The dependence of life scientists on software has steadily grown in recent years. For many tasks, researchers have to decide which of the available bioinformatics software are more suitable for their specific needs. Additionally researchers should be able to objectively select the software that provides the highest accuracy, the best efficiency and the highest level of reproducibility when integrated in their research projects.Critical benchmarking of bioinformatics methods, tools and web services is therefore an essential community service, as well as a critical component of reproducibility efforts. Unbiased and objective evaluations are challenging to set up and can only be effective when built and implemented around community driven efforts, as demonstrated by the many ongoing community challenges in bioinformatics that followed the success of CASP. Community challenges bring the combined benefits of intense collaboration, transparency and standard harmonisation. Only open systems for the continuous evaluation of methods offer a perfect complement to community challenges, offering to larger communities of users that could extend far beyond the community of developers, a window to the developments status that they can use for their specific projects. We understand by continuous evaluation systems as those services which are always available and periodically update their data and/or metrics according to a predefined schedule keeping in mind that the performance has to be always seen in terms of each research domain.We argue here that technology is now mature to bring community driven benchmarking efforts to a higher level that should allow effective interoperability of benchmarks across related methods. New technological developments allow overcoming the limitations of the first experiences on online benchmarking e.g. EVA. We therefore describe OpenEBench, a novel infra-structure designed to establish a continuous automated benchmarking system for bioinformatics methods, tools and web services.OpenEBench is being developed so as to cater for the needs of the bioinformatics community, especially software developers who need an objective and quantitative way to inform their decisions as well as the larger community of end-users, in their search for unbiased and up-to-date evaluation of bioinformatics methods. As such OpenEBench should soon become a central place for bioinformatics software developers, community-driven benchmarking initiatives, researchers using bioinformatics methods, and funders interested in the result of methods evaluation.},
	URL = {https://www.biorxiv.org/content/early/2017/08/31/181677},
	eprint = {https://www.biorxiv.org/content/early/2017/08/31/181677.full.pdf},
	journal = {bioRxiv}
}
